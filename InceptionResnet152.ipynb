{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'D:/UETCodeCamp/dataset/dataset/Images/Train'\n",
    "VALIDATE_PATH = 'D:/UETCodeCamp/dataset/dataset/Images/Validate'\n",
    "TEST_PATH = 'D:/UETCodeCamp/dataset/dataset/Images/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Models/InceptionResNetV2'\n",
    "\n",
    "BASE_MODEL_BEST = os.path.join(PATH, 'base_model_best.hdf5')\n",
    "BASE_MODEL_TRAINED = os.path.join(PATH, 'base_model_trained.hdf5')\n",
    "BASE_MODEL_FIG = os.path.join(PATH, 'base_model_fig.jpg')\n",
    "\n",
    "FINE_TUNE_MODEL_BEST = os.path.join(PATH, 'fine_tune_model_best.hdf5')\n",
    "FINE_TUNE_MODEL_TRAINED = os.path.join(PATH, 'fine_tune_model_trained.hdf5')\n",
    "FINE_TUNE_MODE_FIG = os.path.join(PATH, 'fine_tune_model_fig.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (300, 300)\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 40, \n",
    "    width_shift_range = 0.2, \n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "validate_generator = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18751 images belonging to 38 classes.\n",
      "Found 2757 images belonging to 38 classes.\n",
      "Found 5169 images belonging to 38 classes.\n"
     ]
    }
   ],
   "source": [
    "generated_train_data = train_generator.flow_from_directory(TRAIN_PATH, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE)\n",
    "generated_validate_data = validate_generator.flow_from_directory(VALIDATE_PATH, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE)\n",
    "generated_test_data = test_generator.flow_from_directory(TEST_PATH, target_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 38\n",
    "INITIAL_EPOCHS = 15\n",
    "FINE_TUNE_EPOCHS = 15\n",
    "TOTAL_EPOCHS = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n",
    "FINE_TUNE_AT = 711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\VuongQuan14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = InceptionResNetV2(weights='imagenet', include_top=False)\n",
    "last_output = pretrained_model.output\n",
    "x = GlobalAveragePooling2D()(last_output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(CLASSES, activation='softmax')(x)\n",
    "model = Model(inputs=pretrained_model.input, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "base_checkpointer = ModelCheckpoint(\n",
    "    filepath=BASE_MODEL_BEST.replace('.hdf5', '.keras'), \n",
    "    save_best_only=True, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "fine_tune_checkpointer = ModelCheckpoint(\n",
    "    filepath=FINE_TUNE_MODEL_BEST.replace('.hdf5', '.keras'), \n",
    "    save_best_only=True,\n",
    "    verbose=1, \n",
    ")\n",
    "\n",
    "\n",
    "# Stop if no improvement after 3 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VuongQuan14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "c:\\Users\\VuongQuan14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.2461 - loss: 2.8202 \n",
      "Epoch 1: val_loss improved from inf to 1.71868, saving model to Models/InceptionResNetV2\\base_model_best.keras\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2992s\u001b[0m 20s/step - accuracy: 0.2468 - loss: 2.8169 - val_accuracy: 0.4862 - val_loss: 1.7187\n",
      "Epoch 2/15\n",
      "\u001b[1m  1/146\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44:31\u001b[0m 18s/step - accuracy: 0.4219 - loss: 1.8363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VuongQuan14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss did not improve from 1.71868\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 205ms/step - accuracy: 0.4219 - loss: 1.8363 - val_accuracy: 0.4783 - val_loss: 1.7608\n",
      "Epoch 3/15\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.4718 - loss: 1.7854 \n",
      "Epoch 3: val_loss improved from 1.71868 to 1.51496, saving model to Models/InceptionResNetV2\\base_model_best.keras\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2633s\u001b[0m 18s/step - accuracy: 0.4719 - loss: 1.7849 - val_accuracy: 0.5435 - val_loss: 1.5150\n",
      "Epoch 4/15\n",
      "\u001b[1m  1/146\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25:15\u001b[0m 10s/step - accuracy: 0.4766 - loss: 1.7179\n",
      "Epoch 4: val_loss improved from 1.51496 to 1.46623, saving model to Models/InceptionResNetV2\\base_model_best.keras\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - accuracy: 0.4766 - loss: 1.7179 - val_accuracy: 0.5362 - val_loss: 1.4662\n",
      "Epoch 5/15\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - accuracy: 0.5287 - loss: 1.5501 \n",
      "Epoch 5: val_loss improved from 1.46623 to 1.45987, saving model to Models/InceptionResNetV2\\base_model_best.keras\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2300s\u001b[0m 16s/step - accuracy: 0.5287 - loss: 1.5501 - val_accuracy: 0.5699 - val_loss: 1.4599\n",
      "Epoch 6/15\n",
      "\u001b[1m  1/146\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23:36\u001b[0m 10s/step - accuracy: 0.6016 - loss: 1.3484\n",
      "Epoch 6: val_loss did not improve from 1.45987\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6016 - loss: 1.3484 - val_accuracy: 0.5942 - val_loss: 1.5555\n",
      "Epoch 7/15\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.5640 - loss: 1.4539 \n",
      "Epoch 7: val_loss improved from 1.45987 to 1.35130, saving model to Models/InceptionResNetV2\\base_model_best.keras\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3232s\u001b[0m 22s/step - accuracy: 0.5640 - loss: 1.4538 - val_accuracy: 0.6019 - val_loss: 1.3513\n",
      "Epoch 8/15\n",
      "\u001b[1m  1/146\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23:51\u001b[0m 10s/step - accuracy: 0.5547 - loss: 1.4243\n",
      "Epoch 8: val_loss improved from 1.35130 to 1.13149, saving model to Models/InceptionResNetV2\\base_model_best.keras\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.5547 - loss: 1.4243 - val_accuracy: 0.6667 - val_loss: 1.1315\n",
      "Epoch 9/15\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.5863 - loss: 1.3626 \n",
      "Epoch 9: val_loss did not improve from 1.13149\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1935s\u001b[0m 13s/step - accuracy: 0.5863 - loss: 1.3627 - val_accuracy: 0.6079 - val_loss: 1.3267\n",
      "Epoch 10/15\n",
      "\u001b[1m  1/146\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26:52\u001b[0m 11s/step - accuracy: 0.5469 - loss: 1.5046\n",
      "Epoch 10: val_loss did not improve from 1.13149\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.5469 - loss: 1.5046 - val_accuracy: 0.6957 - val_loss: 1.2208\n",
      "Epoch 11/15\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - accuracy: 0.6027 - loss: 1.3343 \n",
      "Epoch 11: val_loss did not improve from 1.13149\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1925s\u001b[0m 13s/step - accuracy: 0.6027 - loss: 1.3343 - val_accuracy: 0.6112 - val_loss: 1.3673\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "for layer in pretrained_model.layers: layer.trainable = False\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    generated_train_data,\n",
    "    validation_data = generated_validate_data,\n",
    "    validation_steps = generated_validate_data.n // BATCH_SIZE,\n",
    "    steps_per_epoch = generated_train_data.n // BATCH_SIZE,\n",
    "    callbacks = [base_checkpointer, early_stopping],\n",
    "    epochs = INITIAL_EPOCHS,\n",
    "    verbose = 1,\n",
    ")\n",
    "model.save(BASE_MODEL_TRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.4288 - loss: 2.1140 \n",
      "Epoch 11: val_loss improved from inf to 1.24048, saving model to Models/InceptionResNetV2\\fine_tune_model_best.keras\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3314s\u001b[0m 23s/step - accuracy: 0.4293 - loss: 2.1117 - val_accuracy: 0.6369 - val_loss: 1.2405\n",
      "Epoch 12/30\n",
      "\u001b[1m  1/146\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m50:05\u001b[0m 21s/step - accuracy: 0.5547 - loss: 1.5444\n",
      "Epoch 12: val_loss improved from 1.24048 to 1.22485, saving model to Models/InceptionResNetV2\\fine_tune_model_best.keras\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 89ms/step - accuracy: 0.5547 - loss: 1.5444 - val_accuracy: 0.6522 - val_loss: 1.2248\n",
      "Epoch 13/30\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.5980 - loss: 1.3921 \n",
      "Epoch 13: val_loss did not improve from 1.22485\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2205s\u001b[0m 15s/step - accuracy: 0.5981 - loss: 1.3919 - val_accuracy: 0.6403 - val_loss: 1.2357\n",
      "Epoch 13: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "for layer in pretrained_model.layers[:FINE_TUNE_AT]: layer.trainable = False\n",
    "for layer in pretrained_model.layers[FINE_TUNE_AT:]: layer.trainable = True\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(\n",
    "    optimizer = SGD(learning_rate=1e-4, momentum=0.9), \n",
    "    loss = 'categorical_crossentropy', \n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "history_fine = model.fit(\n",
    "    generated_train_data,\n",
    "    validation_data = generated_validate_data,\n",
    "    validation_steps = generated_validate_data.n // BATCH_SIZE,\n",
    "    steps_per_epoch = generated_train_data.n // BATCH_SIZE,\n",
    "    epochs = TOTAL_EPOCHS,\n",
    "    initial_epoch = history.epoch[-1],\n",
    "    callbacks = [fine_tune_checkpointer, early_stopping],\n",
    "    verbose = 1,\n",
    ")\n",
    "model.save(FINE_TUNE_MODEL_TRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5169 images belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VuongQuan14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 11s/step\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        Banh beo       0.83      0.71      0.77       129\n",
      "    Banh bot loc       0.65      0.49      0.56       144\n",
      "        Banh can       0.67      0.70      0.69       149\n",
      "       Banh canh       0.42      0.40      0.41       193\n",
      "      Banh chung       0.73      0.73      0.73       102\n",
      "       Banh cuon       0.64      0.65      0.65       228\n",
      "        Banh duc       0.48      0.21      0.29       133\n",
      "        Banh gio       0.66      0.88      0.76       129\n",
      "       Banh khot       0.81      0.66      0.73       167\n",
      "         Banh mi       0.89      0.93      0.91       268\n",
      "        Banh pia       0.84      0.87      0.85        89\n",
      "        Banh tet       0.81      0.84      0.82       138\n",
      "Banh trang nuong       0.77      0.81      0.79       159\n",
      "        Banh xeo       0.75      0.81      0.78       235\n",
      "      Bun bo Hue       0.51      0.70      0.59       306\n",
      " Bun dau mam tom       0.74      0.94      0.83       184\n",
      "         Bun mam       0.56      0.60      0.58       155\n",
      "        Bun rieu       0.50      0.62      0.56       231\n",
      "  Bun thit nuong       0.65      0.41      0.51       150\n",
      "      Bánh cu đơ       0.44      0.78      0.56        18\n",
      "     Bánh mì cay       0.50      0.57      0.53        14\n",
      "     Bánh đa cua       0.40      0.14      0.21        14\n",
      "   Bánh đậu xanh       0.65      0.88      0.75        17\n",
      "          Bò bía       0.71      0.53      0.61        19\n",
      "          Bún cá       0.00      0.00      0.00        13\n",
      "       Ca kho to       0.83      0.85      0.84       136\n",
      "       Canh chua       0.54      0.59      0.56       165\n",
      "         Cao lau       0.66      0.56      0.61       124\n",
      "       Chao long       0.71      0.66      0.68       215\n",
      "         Com tam       0.72      0.79      0.75       189\n",
      "        Cơm cháy       0.73      0.61      0.67        18\n",
      "        Goi cuon       0.84      0.78      0.81       172\n",
      "         Hu tieu       0.52      0.33      0.40       197\n",
      "        Mi quang       0.64      0.57      0.60       177\n",
      "        Nem chua       0.53      0.44      0.48       109\n",
      "       Nem nướng       0.60      0.75      0.67        16\n",
      "             Pho       0.43      0.48      0.45       162\n",
      "         Xoi xeo       0.82      0.80      0.81       105\n",
      "\n",
      "        accuracy                           0.66      5169\n",
      "       macro avg       0.64      0.63      0.63      5169\n",
      "    weighted avg       0.66      0.66      0.66      5169\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VuongQuan14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\VuongQuan14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\VuongQuan14\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sử dụng generator để dự đoán nhãn cho dữ liệu kiểm tra\n",
    "generated_test_data = test_generator.flow_from_directory(TEST_PATH, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Dự đoán nhãn\n",
    "predictions = model.predict(generated_test_data)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = generated_test_data.classes\n",
    "\n",
    "# Tính toán và in ra các chỉ số\n",
    "class_labels = list(generated_test_data.class_indices.keys())\n",
    "report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
